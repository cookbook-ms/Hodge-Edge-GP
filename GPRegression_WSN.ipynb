{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Regression on Nodes and Edges in a Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('..') \n",
    "from utils.preprocessing import load_dataset\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Positive\n",
    "\n",
    "from kernels.kernel_wsn import HodgeDiffusionKernel, HodgeMaternKernel, HodgeDiffusionKernelNonHC, HodgeMaternKernelNonHC\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the Dataset\n",
    "\n",
    "Here we standardize the data around the center and 0 with a standard deviation of 1. This is important when we have node and edge signals which are of different units. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'water_network'\n",
    "kernel_name = 'matern'\n",
    "\n",
    "seed = 2\n",
    "train_ratio=0.5\n",
    "\n",
    "incidence_matrices, laplacians, data_train, data_test, data, train_ids, test_ids, hr = load_dataset(data_name, train_ratio=train_ratio, spinors=True, seed=seed)\n",
    "\n",
    "if data_name in ['water_network']: sc_order = 1\n",
    "\n",
    "x_train, y_train = torch.from_numpy(data_train[0]).float(), torch.from_numpy(data_train[1]).float()\n",
    "x_test, y_test = torch.from_numpy(data_test[0]).float(), torch.from_numpy(data_test[1]).float()\n",
    "x, y = torch.from_numpy(data[0]).float(), torch.from_numpy(data[1]).float()\n",
    "\n",
    "orig_mean, orig_std = torch.mean(y_train), torch.std(y_train)\n",
    "y_train = (y_train-orig_mean)/orig_std\n",
    "y_test = (y_test-orig_mean)/orig_std\n",
    "y = (y-orig_mean)/orig_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sc_order == 1: \n",
    "    incidence_matrices = torch.from_numpy(incidence_matrices).float()\n",
    "    hr = torch.from_numpy(hr).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move data to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if sc_order == 1:\n",
    "        incidence_matrices = incidence_matrices.cuda()\n",
    "        hr = hr.cuda()\n",
    "    x_train, y_train = x_train.cuda(), y_train.cuda()\n",
    "    x_test, y_test = x_test.cuda(), y_test.cuda()\n",
    "    x, y = x.cuda(), y.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernels\n",
    "Here we express the kernels in terms of the python variables. \n",
    "Given the eigendecompositions:\n",
    "$$L_0 = U_0 \\Lambda_0 U_0^\\top \\quad L_1 = U_1\\Lambda_1 U_1^\\top$$\n",
    "\n",
    "##### diffusion-HC\n",
    "$$\n",
    "    K_0= \\gamma_{down} U_0 e^{-\\kappa_{down} \\Lambda_{0}} U_0^\\top  \\\\\n",
    "    K_1 = \\gamma_{up} B_1^\\top K_0 B_1 \n",
    "$$\n",
    "\n",
    "##### diffusion-NonHC\n",
    "$$\n",
    "    K_0 = \\gamma_{down} U_0 e^{-\\kappa_{down} \\Lambda_{0}} U_0^\\top \\\\\n",
    "    K_1 = \\gamma_{up} U_1 e^{-\\kappa_{up} \\Lambda_{1}} U_1^\\top \n",
    "$$\n",
    "\n",
    "##### Matern-HC \n",
    "$$\n",
    "    K_0 = \\gamma_{down} U_0 (\\frac{2\\kappa_{up}}{\\kappa_{down}^2} + \\Lambda_0) U_0^\\top \\\\\n",
    "    K_1 = \\gamma_{up} B_1^\\top K_0 B_1 \n",
    "$$\n",
    "\n",
    "##### Matern-NonHC \n",
    "$$\n",
    "    K_0 = \\gamma_{down} U_0 (\\frac{2\\mu_{down}}{\\kappa_{down}^2} + \\Lambda_0) U_0^\\top \\\\\n",
    "    K_1 = \\gamma_{up} U_1 (\\frac{2\\mu_{up}}{\\kappa_{up}^2} + \\Lambda_1) U_1^\\top \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a GPR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel, mean_function=None):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        if mean_function is None:\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "        elif mean_function == 'zero':\n",
    "            self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(kernel, outputscale_constraint=Positive())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x) \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "if kernel_name == 'diffusion-nonhc':\n",
    "    kernel = HodgeDiffusionKernelNonHC(incidence_matrices)\n",
    "elif kernel_name == 'diffusion':\n",
    "    kernel = HodgeDiffusionKernel(incidence_matrices)\n",
    "elif kernel_name == 'matern':\n",
    "    kernel = HodgeMaternKernel(incidence_matrices)\n",
    "elif kernel_name == 'matern-nonhc':\n",
    "    kernel = HodgeMaternKernelNonHC(incidence_matrices)\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(x_train, y_train, likelihood, kernel, mean_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "   model = model.cuda()\n",
    "   likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:50} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the hyperprameters\n",
    "we consider random initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel_name in ['diffusion','diffusion-nonhc']:\n",
    "#     hypers = {\n",
    "#         'likelihood.noise_covar.noise': torch.tensor(1.),\n",
    "#         'covar_module.base_kernel.kappa_down': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.kappa_up': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.gamma_down': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.gamma_up': torch.tensor(1),\n",
    "#         'covar_module.outputscale': torch.tensor(1.),\n",
    "#         'mean_module.raw_constant': orig_mean,\n",
    "#     }\n",
    "# elif kernel_name in ['matern','matern-nonhc']:\n",
    "#     hypers = {\n",
    "#         'likelihood.noise_covar.noise': torch.tensor(1.),\n",
    "#         'covar_module.base_kernel.kappa_down': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.kappa_up': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.gamma_down': torch.tensor(1),\n",
    "#         'covar_module.base_kernel.gamma_up': torch.tensor(1),\n",
    "#         'covar_module.outputscale': torch.tensor(1.),\n",
    "#         'mean_module.raw_constant': orig_mean,\n",
    "#     }\n",
    "\n",
    "# model.initialize(**hypers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GPR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap training, prediction and plotting from the ExactGP-Tutorial into a function,\n",
    "# so that we do not have to repeat the code later on\n",
    "def train(model, likelihood, training_iter=training_iter):\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    iterator = tqdm(range(training_iter))\n",
    "    for i in iterator:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(x_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "                i + 1, training_iter, loss.item()\n",
    "        ))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model the analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "train(model, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likelihood, x_test):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        return likelihood(model(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "observed_pred = predict(model, likelihood, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_var = observed_pred.mean, observed_pred.variance\n",
    "lower, upper = observed_pred.confidence_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes, test_edges = len(test_ids[0]), len(test_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse0 = torch.linalg.norm(y_test[:test_nodes] - pred_mean[:test_nodes])**2/test_nodes\n",
    "mae0 = gpytorch.metrics.mean_absolute_error(observed_pred[:test_nodes], y_test[:test_nodes])\n",
    "nlpd0 = gpytorch.metrics.negative_log_predictive_density(observed_pred[:test_nodes], y_test[:test_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1 = torch.linalg.norm(y_test[test_nodes:] - pred_mean[test_nodes:])**2/test_edges\n",
    "mae1 = gpytorch.metrics.mean_absolute_error(observed_pred[test_nodes:], y_test[test_nodes:] )\n",
    "nlpd1 = gpytorch.metrics.negative_log_predictive_density(observed_pred[test_nodes:], y_test[test_nodes:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = predict(model, likelihood, x)\n",
    "all_mean, all_var = pred_all.mean, pred_all.variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the metrics \n",
    "print(f'MSE: {mse0.item():.4f} (node), {mse1.item():.4f} (edge)' '\\n'\n",
    "      f'MAE: {mae0.item():.4f} (node), {mae1.item():.4f} (edge)' '\\n'\n",
    "      f'NLPD: {nlpd0.item():.4f} (node), {nlpd1.item():.4f} (edge)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

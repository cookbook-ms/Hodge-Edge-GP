{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Regression on Edges of a Simplicial Complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "import numpy as np\n",
    "from utils.preprocessing import load_dataset\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Positive\n",
    "from kernels.edge_kernel_ocean_flow import DiffusionKernelOceanFlow, MaternKernelOceanFlow, DiffusionKernelOceanFlowNonHC, MaternKernelOceanFlowNonHC, LaplacianKernelOceanFlow, LaplacianKernelOceanFlowNonHC\n",
    "\n",
    "from tqdm.notebook import tqdm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_normalization = False \n",
    "\n",
    "kernel_name = ['diffusion','matern','diffusion-nonhc','matern-nonhc','laplacian','laplacian-nonhc']\n",
    "\n",
    "kernel_name = 'diffusion'\n",
    "seed = 5\n",
    "\n",
    "incidence_matrices, laplacians, data_train, data_test, data, eigenpairs = load_dataset(data_name='ocean_flow', train_ratio=0.2, seed=seed)\n",
    "eigvecs, eigvals = eigenpairs\n",
    "L1, L1_down, L1_up = laplacians\n",
    "B1, B2 = incidence_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = []\n",
    "total_div = []\n",
    "total_curl = []\n",
    "num_eigemodes = len(eigvals)\n",
    "for i in range(num_eigemodes):\n",
    "    total_var.append(eigvecs[:, i].T@L1@eigvecs[:, i])\n",
    "    total_div.append(eigvecs[:, i].T@L1_down@eigvecs[:, i])\n",
    "    total_curl.append(eigvecs[:, i].T@L1_up@eigvecs[:, i])\n",
    "    \n",
    "grad_eflow = np.where(np.array(total_div) >= 1e-4)[0]\n",
    "curl_eflow = np.where(np.array(total_curl) >= 1e-3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2036278/742530486.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:269.)\n",
      "  L1 = torch.sparse_coo_tensor(L1.nonzero(), L1.data, L1.shape)\n"
     ]
    }
   ],
   "source": [
    "# tensor data type \n",
    "data_type = torch.float32\n",
    "# convert the laplacians first to a list then to torch sparse csr tensors\n",
    "L1 = torch.sparse_coo_tensor(L1.nonzero(), L1.data, L1.shape)\n",
    "L1_down = torch.sparse_coo_tensor(L1_down.nonzero(), L1_down.data, L1_down.shape)\n",
    "L1_up = torch.sparse_coo_tensor(L1_up.nonzero(), L1_up.data, L1_up.shape)\n",
    "grad_evectors = torch.tensor(eigvecs[:, grad_eflow], dtype=data_type)\n",
    "curl_evectors = torch.tensor(eigvecs[:, curl_eflow], dtype=data_type)\n",
    "grad_evalues = torch.tensor(eigvals[grad_eflow], dtype=data_type)\n",
    "curl_evalues = torch.tensor(eigvals[curl_eflow], dtype=data_type)\n",
    "laplacians = [L1, L1_down, L1_up]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Perprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = torch.tensor(data_train[0], dtype=data_type) , torch.tensor(data_train[1], dtype=data_type) \n",
    "x_test, y_test = torch.tensor(data_test[0], dtype=data_type) , torch.tensor(data_test[1], dtype=data_type)\n",
    "x, y = torch.tensor(data[0], dtype=data_type), torch.tensor(data[1], dtype=data_type )\n",
    "\n",
    "orig_mean, orig_std = torch.mean(y_train), torch.std(y_train)\n",
    "\n",
    "if data_normalization:\n",
    "    y_train = (y_train-orig_mean)/orig_std\n",
    "    y_test = (y_test-orig_mean)/orig_std\n",
    "    y = (y-orig_mean)/orig_std\n",
    "\n",
    "output_device = torch.device('cuda:1')\n",
    "x_train, y_train = x_train.to(output_device), y_train.to(output_device)\n",
    "x_test, y_test = x_test.to(output_device), y_test.to(output_device)\n",
    "x, y = x.to(output_device), y.to(output_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to device\n",
    "laplacians = [laplacian.to(output_device) for laplacian in laplacians]\n",
    "# convert the eigenpairs to torch tensors\n",
    "grad_evectors = grad_evectors.to(output_device)\n",
    "curl_evectors = curl_evectors.to(output_device)\n",
    "grad_evalues = grad_evalues.to(output_device)\n",
    "curl_evalues = curl_evalues.to(output_device)\n",
    "eigenpairs = [grad_evectors, curl_evectors, grad_evalues, curl_evalues]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Kernel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel, mean_function=None):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        if mean_function is None:\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "        elif mean_function == 'zero':\n",
    "            self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(kernel, outputscale_constraint=Positive())\n",
    "        # self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "if kernel_name == 'diffusion':\n",
    "    kernel = DiffusionKernelOceanFlow(eigenpairs)\n",
    "elif kernel_name == 'diffusion-nonhc':\n",
    "    kernel = DiffusionKernelOceanFlowNonHC(eigenpairs)\n",
    "elif kernel_name == 'matern':\n",
    "    kernel = MaternKernelOceanFlow(eigenpairs)\n",
    "elif kernel_name == 'matern-nonhc':\n",
    "    kernel = MaternKernelOceanFlowNonHC(eigenpairs)\n",
    "elif kernel_name == 'laplacian':\n",
    "    kernel = LaplacianKernelOceanFlow(eigenpairs)\n",
    "elif kernel_name == 'laplacian-nonhc':\n",
    "    kernel = LaplacianKernelOceanFlowNonHC(eigenpairs)\n",
    "elif kernel_name == 'naive-matern':\n",
    "    kernel = gpytorch.kernels.MaternKernel()\n",
    "elif kernel_name == 'naive-rbf':\n",
    "    kernel = gpytorch.kernels.RBFKernel()\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(x_train, y_train, likelihood, kernel, mean_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "   model = model.to(output_device)\n",
    "   likelihood = likelihood.to(output_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise                   value = 0.0\n",
      "Parameter name: mean_module.raw_constant                           value = 0.0\n",
      "Parameter name: covar_module.raw_outputscale                       value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_kappa_down            value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_kappa_up              value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_h_down                value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_h_up                  value = 0.0\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:50} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the hyperprameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypers = {\n",
    "#     'likelihood.noise_covar.noise': torch.tensor(1.),\n",
    "#     'covar_module.base_kernel.kappa_down': torch.tensor(1),\n",
    "#     'covar_module.base_kernel.kappa_up': torch.tensor(1),\n",
    "#     # 'covar_module.base_kernel.mu': torch.tensor(1),\n",
    "#     'covar_module.outputscale': torch.tensor(1.),\n",
    "#     'mean_module.raw_constant': orig_mean,\n",
    "# }\n",
    "\n",
    "# model.initialize(**hypers)\n",
    "# print(\n",
    "#     model.likelihood.noise_covar.noise.item(),\n",
    "#     model.covar_module.base_kernel.kappa_down.item(),\n",
    "#     model.covar_module.base_kernel.kappa_up.item(),\n",
    "#     # model.covar_module.base_kernel.mu.item(),\n",
    "#     model.covar_module.outputscale.item(),\n",
    "#     model.mean_module.constant.item()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GPR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap training, prediction and plotting from the ExactGP-Tutorial into a function,\n",
    "# so that we do not have to repeat the code later on\n",
    "def train(model, likelihood, training_iter=training_iter):\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    iterator = tqdm(range(training_iter))\n",
    "    for i in iterator:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(x_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "        ))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model the analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c72f4bb298e4a0aa59286e2d8a03bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 1.383\n",
      "Iter 2/200 - Loss: 1.373\n",
      "Iter 3/200 - Loss: 1.361\n",
      "Iter 4/200 - Loss: 1.353\n",
      "Iter 5/200 - Loss: 1.345\n",
      "Iter 6/200 - Loss: 1.334\n",
      "Iter 7/200 - Loss: 1.320\n",
      "Iter 8/200 - Loss: 1.305\n",
      "Iter 9/200 - Loss: 1.288\n",
      "Iter 10/200 - Loss: 1.268\n",
      "Iter 11/200 - Loss: 1.244\n",
      "Iter 12/200 - Loss: 1.218\n",
      "Iter 13/200 - Loss: 1.189\n",
      "Iter 14/200 - Loss: 1.159\n",
      "Iter 15/200 - Loss: 1.126\n",
      "Iter 16/200 - Loss: 1.088\n",
      "Iter 17/200 - Loss: 1.049\n",
      "Iter 18/200 - Loss: 1.009\n",
      "Iter 19/200 - Loss: 0.968\n",
      "Iter 20/200 - Loss: 0.927\n",
      "Iter 21/200 - Loss: 0.886\n",
      "Iter 22/200 - Loss: 0.842\n",
      "Iter 23/200 - Loss: 0.802\n",
      "Iter 24/200 - Loss: 0.767\n",
      "Iter 25/200 - Loss: 0.728\n",
      "Iter 26/200 - Loss: 0.696\n",
      "Iter 27/200 - Loss: 0.666\n",
      "Iter 28/200 - Loss: 0.639\n",
      "Iter 29/200 - Loss: 0.620\n",
      "Iter 30/200 - Loss: 0.605\n",
      "Iter 31/200 - Loss: 0.598\n",
      "Iter 32/200 - Loss: 0.591\n",
      "Iter 33/200 - Loss: 0.582\n",
      "Iter 34/200 - Loss: 0.588\n",
      "Iter 35/200 - Loss: 0.586\n",
      "Iter 36/200 - Loss: 0.578\n",
      "Iter 37/200 - Loss: 0.584\n",
      "Iter 38/200 - Loss: 0.592\n",
      "Iter 39/200 - Loss: 0.589\n",
      "Iter 40/200 - Loss: 0.570\n",
      "Iter 41/200 - Loss: 0.556\n",
      "Iter 42/200 - Loss: 0.556\n",
      "Iter 43/200 - Loss: 0.550\n",
      "Iter 44/200 - Loss: 0.545\n",
      "Iter 45/200 - Loss: 0.541\n",
      "Iter 46/200 - Loss: 0.542\n",
      "Iter 47/200 - Loss: 0.535\n",
      "Iter 48/200 - Loss: 0.529\n",
      "Iter 49/200 - Loss: 0.535\n",
      "Iter 50/200 - Loss: 0.539\n",
      "Iter 51/200 - Loss: 0.534\n",
      "Iter 52/200 - Loss: 0.530\n",
      "Iter 53/200 - Loss: 0.531\n",
      "Iter 54/200 - Loss: 0.533\n",
      "Iter 55/200 - Loss: 0.518\n",
      "Iter 56/200 - Loss: 0.521\n",
      "Iter 57/200 - Loss: 0.521\n",
      "Iter 58/200 - Loss: 0.518\n",
      "Iter 59/200 - Loss: 0.510\n",
      "Iter 60/200 - Loss: 0.511\n",
      "Iter 61/200 - Loss: 0.512\n",
      "Iter 62/200 - Loss: 0.512\n",
      "Iter 63/200 - Loss: 0.505\n",
      "Iter 64/200 - Loss: 0.503\n",
      "Iter 65/200 - Loss: 0.508\n",
      "Iter 66/200 - Loss: 0.500\n",
      "Iter 67/200 - Loss: 0.496\n",
      "Iter 68/200 - Loss: 0.496\n",
      "Iter 69/200 - Loss: 0.488\n",
      "Iter 70/200 - Loss: 0.494\n",
      "Iter 71/200 - Loss: 0.488\n",
      "Iter 72/200 - Loss: 0.492\n",
      "Iter 73/200 - Loss: 0.488\n",
      "Iter 74/200 - Loss: 0.487\n",
      "Iter 75/200 - Loss: 0.487\n",
      "Iter 76/200 - Loss: 0.480\n",
      "Iter 77/200 - Loss: 0.483\n",
      "Iter 78/200 - Loss: 0.482\n",
      "Iter 79/200 - Loss: 0.484\n",
      "Iter 80/200 - Loss: 0.480\n",
      "Iter 81/200 - Loss: 0.474\n",
      "Iter 82/200 - Loss: 0.477\n",
      "Iter 83/200 - Loss: 0.478\n",
      "Iter 84/200 - Loss: 0.474\n",
      "Iter 85/200 - Loss: 0.465\n",
      "Iter 86/200 - Loss: 0.468\n",
      "Iter 87/200 - Loss: 0.468\n",
      "Iter 88/200 - Loss: 0.468\n",
      "Iter 89/200 - Loss: 0.463\n",
      "Iter 90/200 - Loss: 0.465\n",
      "Iter 91/200 - Loss: 0.464\n",
      "Iter 92/200 - Loss: 0.465\n",
      "Iter 93/200 - Loss: 0.466\n",
      "Iter 94/200 - Loss: 0.464\n",
      "Iter 95/200 - Loss: 0.463\n",
      "Iter 96/200 - Loss: 0.460\n",
      "Iter 97/200 - Loss: 0.460\n",
      "Iter 98/200 - Loss: 0.465\n",
      "Iter 99/200 - Loss: 0.464\n",
      "Iter 100/200 - Loss: 0.466\n",
      "Iter 101/200 - Loss: 0.463\n",
      "Iter 102/200 - Loss: 0.458\n",
      "Iter 103/200 - Loss: 0.461\n",
      "Iter 104/200 - Loss: 0.455\n",
      "Iter 105/200 - Loss: 0.455\n",
      "Iter 106/200 - Loss: 0.459\n",
      "Iter 107/200 - Loss: 0.455\n",
      "Iter 108/200 - Loss: 0.457\n",
      "Iter 109/200 - Loss: 0.454\n",
      "Iter 110/200 - Loss: 0.455\n",
      "Iter 111/200 - Loss: 0.452\n",
      "Iter 112/200 - Loss: 0.457\n",
      "Iter 113/200 - Loss: 0.453\n",
      "Iter 114/200 - Loss: 0.455\n",
      "Iter 115/200 - Loss: 0.451\n",
      "Iter 116/200 - Loss: 0.452\n",
      "Iter 117/200 - Loss: 0.459\n",
      "Iter 118/200 - Loss: 0.453\n",
      "Iter 119/200 - Loss: 0.459\n",
      "Iter 120/200 - Loss: 0.451\n",
      "Iter 121/200 - Loss: 0.449\n",
      "Iter 122/200 - Loss: 0.451\n",
      "Iter 123/200 - Loss: 0.450\n",
      "Iter 124/200 - Loss: 0.449\n",
      "Iter 125/200 - Loss: 0.454\n",
      "Iter 126/200 - Loss: 0.453\n",
      "Iter 127/200 - Loss: 0.450\n",
      "Iter 128/200 - Loss: 0.450\n",
      "Iter 129/200 - Loss: 0.449\n",
      "Iter 130/200 - Loss: 0.453\n",
      "Iter 131/200 - Loss: 0.446\n",
      "Iter 132/200 - Loss: 0.451\n",
      "Iter 133/200 - Loss: 0.447\n",
      "Iter 134/200 - Loss: 0.451\n",
      "Iter 135/200 - Loss: 0.444\n",
      "Iter 136/200 - Loss: 0.448\n",
      "Iter 137/200 - Loss: 0.447\n",
      "Iter 138/200 - Loss: 0.451\n",
      "Iter 139/200 - Loss: 0.450\n",
      "Iter 140/200 - Loss: 0.449\n",
      "Iter 141/200 - Loss: 0.444\n",
      "Iter 142/200 - Loss: 0.441\n",
      "Iter 143/200 - Loss: 0.446\n",
      "Iter 144/200 - Loss: 0.445\n",
      "Iter 145/200 - Loss: 0.446\n",
      "Iter 146/200 - Loss: 0.443\n",
      "Iter 147/200 - Loss: 0.444\n",
      "Iter 148/200 - Loss: 0.440\n",
      "Iter 149/200 - Loss: 0.437\n",
      "Iter 150/200 - Loss: 0.441\n",
      "Iter 151/200 - Loss: 0.443\n",
      "Iter 152/200 - Loss: 0.443\n",
      "Iter 153/200 - Loss: 0.438\n",
      "Iter 154/200 - Loss: 0.446\n",
      "Iter 155/200 - Loss: 0.442\n",
      "Iter 156/200 - Loss: 0.443\n",
      "Iter 157/200 - Loss: 0.443\n",
      "Iter 158/200 - Loss: 0.438\n",
      "Iter 159/200 - Loss: 0.443\n",
      "Iter 160/200 - Loss: 0.445\n",
      "Iter 161/200 - Loss: 0.442\n",
      "Iter 162/200 - Loss: 0.443\n",
      "Iter 163/200 - Loss: 0.442\n",
      "Iter 164/200 - Loss: 0.441\n",
      "Iter 165/200 - Loss: 0.434\n",
      "Iter 166/200 - Loss: 0.440\n",
      "Iter 167/200 - Loss: 0.438\n",
      "Iter 168/200 - Loss: 0.442\n",
      "Iter 169/200 - Loss: 0.436\n",
      "Iter 170/200 - Loss: 0.435\n",
      "Iter 171/200 - Loss: 0.435\n",
      "Iter 172/200 - Loss: 0.439\n",
      "Iter 173/200 - Loss: 0.440\n",
      "Iter 174/200 - Loss: 0.439\n",
      "Iter 175/200 - Loss: 0.436\n",
      "Iter 176/200 - Loss: 0.437\n",
      "Iter 177/200 - Loss: 0.437\n",
      "Iter 178/200 - Loss: 0.444\n",
      "Iter 179/200 - Loss: 0.441\n",
      "Iter 180/200 - Loss: 0.442\n",
      "Iter 181/200 - Loss: 0.440\n",
      "Iter 182/200 - Loss: 0.438\n",
      "Iter 183/200 - Loss: 0.434\n",
      "Iter 184/200 - Loss: 0.437\n",
      "Iter 185/200 - Loss: 0.437\n",
      "Iter 186/200 - Loss: 0.440\n",
      "Iter 187/200 - Loss: 0.435\n",
      "Iter 188/200 - Loss: 0.435\n",
      "Iter 189/200 - Loss: 0.440\n",
      "Iter 190/200 - Loss: 0.439\n",
      "Iter 191/200 - Loss: 0.436\n",
      "Iter 192/200 - Loss: 0.436\n",
      "Iter 193/200 - Loss: 0.440\n",
      "Iter 194/200 - Loss: 0.436\n",
      "Iter 195/200 - Loss: 0.434\n",
      "Iter 196/200 - Loss: 0.437\n",
      "Iter 197/200 - Loss: 0.438\n",
      "Iter 198/200 - Loss: 0.437\n",
      "Iter 199/200 - Loss: 0.436\n",
      "Iter 200/200 - Loss: 0.436\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "train(model, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likelihood, x_test):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        return likelihood(model(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pred = predict(model, likelihood, x_test)\n",
    "pred_mean, pred_var = observed_pred.mean, observed_pred.variance\n",
    "lower, upper = observed_pred.confidence_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.2513890266418457 \n",
      "Test MSE: 0.11903209984302521 \n",
      "Test NLPD: 0.3344270586967468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Test MAE: {}'.format(gpytorch.metrics.mean_absolute_error(observed_pred, y_test)), '\\n'\n",
    "    'Test MSE: {}'.format(gpytorch.metrics.mean_squared_error(observed_pred, y_test)), '\\n'\n",
    "    'Test NLPD: {}'.format(gpytorch.metrics.negative_log_predictive_density(observed_pred, y_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise',\n",
       "              tensor([-2.2917], device='cuda:1')),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04, device='cuda:1')),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1')),\n",
       "             ('mean_module.raw_constant', tensor(0.0077, device='cuda:1')),\n",
       "             ('covar_module.raw_outputscale', tensor(7.5567, device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_down',\n",
       "              tensor([[8.1434]], device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_up',\n",
       "              tensor([[7.2782]], device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_down',\n",
       "              tensor([[7.2791]], device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_up',\n",
       "              tensor([[7.6236]], device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_down_constraint.lower_bound',\n",
       "              tensor(0., device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_down_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_up_constraint.lower_bound',\n",
       "              tensor(0., device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_kappa_up_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_down_constraint.lower_bound',\n",
       "              tensor(0., device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_down_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_up_constraint.lower_bound',\n",
       "              tensor(0., device='cuda:1')),\n",
       "             ('covar_module.base_kernel.raw_h_up_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1')),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0., device='cuda:1')),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf, device='cuda:1'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
